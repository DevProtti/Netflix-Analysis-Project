{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType\n",
    "from pyspark.sql.functions import col, round, mean, sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the spark environment\n",
    "os.environ[\"JAVA_HOME\"] = 'C:\\Gabriel\\Programming\\Java\\jdk'\n",
    "os.environ[\"SPARK_HOME\"] = 'C:\\Gabriel\\Programming\\Spark\\spark-3.5.0-bin-hadoop3'\n",
    "\n",
    "# Initialize findspark lib\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master('local')\\\n",
    "        .appName(\"NFLX_Report\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming excel sheets in csv and splitting in 3 different files\n",
    "\n",
    "def transform_excel_to_csv(source_path, source_file_name, sheet_name, new_file_name):\n",
    "    source_file_path = os.path.join(source_path, source_file_name)\n",
    "    pandas_df = pd.read_excel(source_file_path, sheet_name=sheet_name)\n",
    "    pandas_df.to_csv(f'{source_path}/{new_file_name}.csv', header=True, index=False)\n",
    "    \n",
    "\n",
    "source_path = 'data'\n",
    "source_file_name = \"NFLX_DS_10_23_Data.xlsx\"\n",
    "\n",
    "# Creating a csv file for NFLX Top 10 sheet\n",
    "excel_sheet = 'NFLX Top 10'\n",
    "transform_excel_to_csv(source_path, source_file_name, excel_sheet, 'NFLX_Top_10')\n",
    "\n",
    "# Creating a csv file for IMDB Rating sheet\n",
    "excel_sheet = 'IMDB Rating'\n",
    "transform_excel_to_csv(source_path, source_file_name, excel_sheet, 'IMDB_Rating')\n",
    "\n",
    "# Creating a csv file for Runtime sheet\n",
    "excel_sheet = 'Runtime'\n",
    "transform_excel_to_csv(source_path, source_file_name, excel_sheet, 'Runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st Question:\n",
    "    > Topics:\n",
    "        - Read the dataset NFLX Top 10\n",
    "        - Filter the data (ignoring the week of May 22nd and getting only TV (English) category)\n",
    "        - Count how many times each show title appeared in the dataset\n",
    "        - Select the title with the most appearances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|          show_title|count|\n",
      "+--------------------+-----+\n",
      "|     Stranger Things|   20|\n",
      "|          Bridgerton|   14|\n",
      "|               Ozark|    6|\n",
      "|Anatomy of a Scandal|    5|\n",
      "|  The Lincoln Lawyer|    5|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "The most frequent English-language TV show in the NFLX Top 10 dataset was \"Stranger Things\" which appeared 20 times.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reading Top 10 csv files with spark\n",
    "file_name = 'NFLX_Top_10'\n",
    "df_top10 = spark.read.option(\"header\", 'true').csv(f'./data/{file_name}.csv', inferSchema=True)\n",
    "\n",
    "outage_week = '2022-05-22'\n",
    "category = 'TV (English)'\n",
    "\n",
    "# Filtering the dataset to exclude the week of May 22nd and retain only entries from the TV (English) category\n",
    "df_top10_filtered = df_top10.filter((df_top10['week'] != outage_week) \n",
    "                                                & (df_top10['category'] == category))\n",
    "\n",
    "# Calculating the amout of times each show title appeared in top 10 dataset\n",
    "result_set_df = df_top10_filtered.groupBy('show_title').count()\n",
    "result_set_df.orderBy(result_set_df['count'].desc()).show(5)\n",
    "most_appeared_tv_show = result_set_df.orderBy(result_set_df['count'].desc()).first()    \n",
    "print(f\"The most frequent English-language TV show in the NFLX Top 10 dataset was \"\\\n",
    "      f\"\\\"{most_appeared_tv_show['show_title']}\\\" which appeared {most_appeared_tv_show['count']} times.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd Question\n",
    "\n",
    "    > Topics:\n",
    "        - Read the datasets (NFTL_Top_10 and IMDB_Rating)\n",
    "        - Filter the data (ignoring the week of May 22nd and getting only Films (English) category)\n",
    "        - Relate the datasets by show title name\n",
    "        - Get the title with the lowest rating\n",
    "        - Get the average from weekly_hours_viewed from each week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowest rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------+\n",
      "|      week|        show_title|rating (avg)|\n",
      "+----------+------------------+------------+\n",
      "|2022-05-15|365 Days: This Day|         2.5|\n",
      "+----------+------------------+------------+\n",
      "only showing top 1 row\n",
      "\n",
      "The title with the lowest rating was \"365 Days: This Day\", with a rating of 2.5\n"
     ]
    }
   ],
   "source": [
    "# Filtering the the NFLX Top 10 dataset by excluding the week of May 22nd and\n",
    "# retaining only entries from the TV (English) category\n",
    "category = 'Films (English)'\n",
    "df_top10_filtered = df_top10.filter((df_top10['week'] != outage_week) \n",
    "                                                & (df_top10['category'] == category))\n",
    "# Reading the IMDB Ratings dataset\n",
    "file_name = 'IMDB_Rating'\n",
    "df_imdb_rating = spark.read.option(\"header\", 'true').csv(f'./data/{file_name}.csv', inferSchema=True)\n",
    "\n",
    "# Left joining Netflix Top 10 (filtered) dataset with IMDB ratings dataset\n",
    "result_set = df_top10_filtered.join(df_imdb_rating, \n",
    "                                    df_top10_filtered['show_title']==df_imdb_rating['title'],\n",
    "                                    how='left')\\\n",
    "                              .select([\"week\", \"show_title\", \"weekly_rank\", \"weekly_hours_viewed\", \"rating\"])\n",
    "\n",
    "# Calculating rating average to handle different ratings for the same title\n",
    "df_title_rating = result_set.groupBy([\"week\",\"show_title\"])\\\n",
    "                            .mean(\"rating\")\\\n",
    "                            .withColumnRenamed(\"avg(rating)\", \"rating (avg)\")\n",
    "                            \n",
    "# Filtering where the average is not equals to zero\n",
    "df_title_rating = df_title_rating.filter(df_title_rating[\"rating (avg)\"] != 0)\n",
    "\n",
    "# Getting the title with the lowest rating\n",
    "df_title_rating.orderBy(\"rating (avg)\").show(1)\n",
    "title_with_lowest_rating = df_title_rating.orderBy(\"rating (avg)\").first()\n",
    "print(f\"The title with the lowest rating was \"\\\n",
    "      f\"\\\"{title_with_lowest_rating['show_title']}\\\", with a rating of {title_with_lowest_rating['rating (avg)']}\")\n",
    "\n",
    "\n",
    "## OBS: I could do this without calculating the average to handle different ratings for the same title,\n",
    "## and the result would be the same, but in this particular case I think it is more coherent calculating the average\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average hour viewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|        show_title|average_hours_viewed|\n",
      "+------------------+--------------------+\n",
      "|365 Days: This Day|            38696666|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering the the NFTL Top 10 dataset by excluding the week of May 22nd and\n",
    "# retaining only entries from the Films (English) category\n",
    "category = 'Films (English)'\n",
    "df_top10_filtered = df_top10.filter((df_top10['week'] != outage_week) \n",
    "                                                & (df_top10['category'] == category))\n",
    "# Reading the IMDB Ratings dataset\n",
    "file_name = 'IMDB_Rating'\n",
    "df_imdb_rating = spark.read.option(\"header\", 'true').csv(f'./data/{file_name}.csv', inferSchema=True)\n",
    "\n",
    "# Left joining Netflix Top 10 (filtered) dataset with IMDB ratings dataset\n",
    "result_set = df_top10_filtered.join(df_imdb_rating, \n",
    "                                    df_top10_filtered['show_title']==df_imdb_rating['title'],\n",
    "                                    how='left')\\\n",
    "                              .select([\"week\", \"category\" ,\"show_title\", \"weekly_rank\", \"weekly_hours_viewed\", \"rating\"])\n",
    "\n",
    "result_set.select(\"category\", \"show_title\",'rating').orderBy(result_set['rating']).filter(result_set['rating'] != 0)\n",
    "\n",
    "# Investigating the amout of weeks the show title appeared\n",
    "weeks_show_appeared = result_set.filter(result_set[\"show_title\"] == '365 Days: This Day')\n",
    "\n",
    "# Calculating the average hours viewed\n",
    "average_hours_viewed = weeks_show_appeared.groupBy(\"show_title\").agg(mean(\"weekly_hours_viewed\").cast(LongType()).alias(\"average_hours_viewed\"))\n",
    "average_hours_viewed.show()                           \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "    > Topics:\n",
    "        - Filter the top 10 Dataset to get Films (Non-English) only\n",
    "        - Order by de the cumulative_weeks_in_top_10 de\n",
    "        - Estimate the total viewers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the title that has spent the most time in the top 10 week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------------+-----------------+------------+-----------+--------------------------+-------------------+\n",
      "|date_added|      week|           category|       show_title|season_title|weekly_rank|cumulative_weeks_in_top_10|weekly_hours_viewed|\n",
      "+----------+----------+-------------------+-----------------+------------+-----------+--------------------------+-------------------+\n",
      "|2022-06-28|2022-06-26|Films (Non-English)|Through My Window|        NULL|         10|                        13|            1500000|\n",
      "+----------+----------+-------------------+-----------------+------------+-----------+--------------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "The non English language title that has spent the most weeks in top 10 was \"Through My Window\" with 13 weeks\n"
     ]
    }
   ],
   "source": [
    "outage_week = '2022-05-22'\n",
    "category = 'Films (Non-English)'\n",
    "\n",
    "# Filtering the dataset by the outage week and category\n",
    "file_name = 'NFLX_Top_10'\n",
    "df_top10 = spark.read.option(\"header\", 'true').csv(f'./data/{file_name}.csv', inferSchema=True)\n",
    "df_non_english_films = df_top10.filter((df_top10[\"week\"] != outage_week) & (df_top10[\"category\"] == category))\n",
    "\n",
    "# Displaying only the title that has spent the most week in top 10\n",
    "df_non_english_films.orderBy(df_non_english_films[\"cumulative_weeks_in_top_10\"].desc()).show(1)\n",
    "non_english_title_most_weeks_top_10 = df_non_english_films.orderBy(df_non_english_films[\"cumulative_weeks_in_top_10\"].desc()).first()\n",
    "\n",
    "print(\n",
    "f\"\"\"The non English language title that has spent the most weeks in top 10 \\\n",
    "was \\\"{non_english_title_most_weeks_top_10['show_title']}\\\" with \\\n",
    "{non_english_title_most_weeks_top_10['cumulative_weeks_in_top_10']} weeks\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the amount of viewers for this title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+--------------+\n",
      "|      week|       show_title|weekly_viewers|\n",
      "+----------+-----------------+--------------+\n",
      "|2022-06-26|Through My Window|        775862|\n",
      "|2022-05-01|Through My Window|       1029310|\n",
      "|2022-04-17|Through My Window|       1179310|\n",
      "|2022-04-24|Through My Window|       1137931|\n",
      "|2022-04-03|Through My Window|       1246552|\n",
      "+----------+-----------------+--------------+\n",
      "\n",
      "+-----------------+-------------+\n",
      "|       show_title|Total_viewers|\n",
      "+-----------------+-------------+\n",
      "|Through My Window|      5368965|\n",
      "+-----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering df_non_english_films Data Frame by the title \"Through My Window\"\n",
    "show_title = \"Through My Window\"\n",
    "df_filtered_by_show_title = df_non_english_films.filter(df_non_english_films[\"show_title\"] == show_title)\n",
    "\n",
    "# Reading the Runtime Dataset\n",
    "df_runtime = spark.read.option(\"header\", 'true').csv(\"data/Runtime.csv\")\n",
    "\n",
    "# Filtering the df_runtime by the title\n",
    "df_runtime_filtered_by_title = df_runtime.filter(df_runtime[\"title\"] == show_title)\n",
    "\n",
    "# Calculating the aproximate number of viewers for each week\n",
    "df_weekly_viewers = df_filtered_by_show_title.crossJoin(df_runtime_filtered_by_title) \\\n",
    "                .withColumn(\"weekly_viewers\", round(col(\"weekly_hours_viewed\") / (col(\"runtime\") / 60), 0).cast(\"int\")) \\\n",
    "                .select(\"week\",\"show_title\", \"weekly_viewers\")\n",
    "\n",
    "df_weekly_viewers.show()\n",
    "\n",
    "# Estimating the number of users who watched 'Through My Window' during the period from 2022-04-03 to 2022-06-26\n",
    "amount_of_users = df_weekly_viewers.groupBy(\"show_title\").agg(sum(\"weekly_viewers\").alias(\"Total_viewers\"))\n",
    "amount_of_users.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
